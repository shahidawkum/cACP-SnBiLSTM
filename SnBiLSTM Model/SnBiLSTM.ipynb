{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u-s9G-HRzb3u"
      },
      "outputs": [],
      "source": [
        "## Import necessary modules\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn import metrics\n",
        "#from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve, matthews_corrcoef\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sn\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import csv\n",
        "import math\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import cross_val_score,KFold,cross_val_predict\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "import sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9kCe5sFUzjkS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2332926e-0a36-4637-cbce-5322824d0944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "M8q4Y_PEzlQM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/Research Materials/ACP Post Doc Paper Shahid saib/Feature Encoding/CTF-Train-ACP.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-CS2KLQ-9S55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "509d727c-36a7-4982-ac98-3cc000eb4beb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4247, 344)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Z5JVxeTBznJ5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "0cbb805c-e312-4fe9-ba5b-b58307a01ccd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler()"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StandardScaler()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "# Initialise the Scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# To scale data\n",
        "scaler.fit(df)\n",
        "#from sklearn.preprocessing import StandardScaler\n",
        "# Initialise the Scaler\n",
        "#scaler = StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df.fillna(999, inplace=True)"
      ],
      "metadata": {
        "id": "-B7w2HToKXJG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kKZLM9tl2sd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f904245-12ab-4243-9299-ecaecbe857bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4247, 342)\n",
            "(4247,)\n"
          ]
        }
      ],
      "source": [
        "# Split the dataset into features (X) and labels (y)\n",
        "X = df.iloc[:, 0:342].values\n",
        "y = df.iloc [:, 343].values\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T4FyfPqpqRzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b906f15b-60bf-4efc-dc4c-87d813756022"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3397 850 3397 850\n"
          ]
        }
      ],
      "source": [
        "# Split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "             X, y, test_size = 0.2, random_state=42)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "caCQknbnoD_4"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Generate balanced dataset\n",
        "X, y = make_classification(n_samples=4268, n_classes=2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrC6nlKHBFnZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6c9f662-5762-4f7b-a04d-0fae6e9f428d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2532 634 2532 634\n"
          ]
        }
      ],
      "source": [
        "# Split into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "             X, y, test_size = 0.2, random_state=42)\n",
        "print (len(X_train),len(X_test),len(y_train),len(y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FKC4TIsOj4S9"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
        "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
        "\n",
        "# Normalize the features\n",
        "X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDHgJVcBjqOL"
      },
      "source": [
        "SnBiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qbDgEvBTdwOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97949643-a43f-4a6d-a158-23ffe1ec72db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "22/22 [==============================] - 0s 5ms/step\n",
            "22/22 [==============================] - 0s 5ms/step\n",
            "22/22 [==============================] - 0s 5ms/step\n",
            "22/22 [==============================] - 0s 5ms/step\n",
            "22/22 [==============================] - 0s 5ms/step\n",
            "Accuracy = 0.9042214999377423\n",
            "F1 Score = 0.9091787155666982\n",
            "Precision = 0.882536099728412\n",
            "Recall = 0.9405893816021192\n",
            "Sensitivity = 0.9405893816021192\n",
            "Specificity = 0.8670434522536759\n",
            "MCC = 0.8130330944387254\n",
            "AUC = 0.9567182473336798\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=5)\n",
        "\n",
        "# Define the input shape\n",
        "input_shape = (X_train.shape[1], 1)\n",
        "\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Bidirectional, LSTM, Dense, BatchNormalization\n",
        "import tensorflow as tf\n",
        "\n",
        "model = Sequential([\n",
        "    Bidirectional(LSTM(64, activation='relu', dropout=0.2, return_sequences=False), input_shape=(X.shape[1], 1)),\n",
        "    BatchNormalization(),  # Add BatchNormalization layer here\n",
        "    Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l1(0.01)),\n",
        "    BatchNormalization(),  # Add BatchNormalization layer here\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Initialize lists to store evaluation metrics for each fold\n",
        "accuracy_list = []\n",
        "f1_list = []\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "sensitivity_list = []\n",
        "specificity_list = []\n",
        "mcc_list = []\n",
        "auc_list = []  # Initialize a list for AUC values\n",
        "\n",
        "# Perform 5-fold cross-validation\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=5)\n",
        "for train_index, val_index in kfold.split(X_train):\n",
        "    # Split the data into training and validation sets for the current fold\n",
        "    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "    y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
        "\n",
        "    # Train the model on the current fold\n",
        "    history=model.fit(X_train_fold, y_train_fold, epochs=40, batch_size=32, verbose=0,validation_split=0.1)\n",
        "\n",
        "    # Evaluate the model on the validation set for the current fold\n",
        "    y_val_pred = model.predict(X_val_fold)\n",
        "    y_val_pred_binary = (y_val_pred > 0.5).astype(int)\n",
        "\n",
        "    # Calculate confusion matrix for the current fold\n",
        "    cm = confusion_matrix(y_val_fold, y_val_pred_binary)\n",
        "    # Calculate AUC for the current fold\n",
        "    auc = roc_auc_score(y_val_fold, y_val_pred)  # Use predicted probabilities for AUC calculation\n",
        "    auc_list.append(auc)\n",
        "\n",
        "\n",
        "    # Calculate performance evaluation metrics for the current fold\n",
        "    TN = cm[0, 0]\n",
        "    FP = cm[0, 1]\n",
        "    FN = cm[1, 0]\n",
        "    TP = cm[1, 1]\n",
        "\n",
        "    # Accuracy\n",
        "    accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "    # F1-Score\n",
        "    f1 = 2 * TP / float(2 * TP + FP + FN)\n",
        "    f1_list.append(f1)\n",
        "\n",
        "    # Precision\n",
        "    precision = TP / float(TP + FP)\n",
        "    precision_list.append(precision)\n",
        "\n",
        "    # Recall\n",
        "    recall = TP / float(TP + FN)\n",
        "    recall_list.append(recall)\n",
        "\n",
        "    # Sensitivity\n",
        "    sensitivity = TP / float(TP + FN)\n",
        "    sensitivity_list.append(sensitivity)\n",
        "\n",
        "    # Specificity\n",
        "    specificity = TN / float(TN + FP)\n",
        "    specificity_list.append(specificity)\n",
        "\n",
        "    # MCC (Matthews Correlation Coefficient)\n",
        "    mcc = ((TP * TN) - (FP * FN)) / float((np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN))) or 1)\n",
        "    mcc_list.append(mcc)\n",
        "\n",
        "# Calculate average performance evaluation metrics across all folds\n",
        "avg_accuracy = np.mean(accuracy_list)\n",
        "avg_f1 = np.mean(f1_list)\n",
        "avg_precision = np.mean(precision_list)\n",
        "avg_recall = np.mean(recall_list)\n",
        "avg_sensitivity = np.mean(sensitivity_list)\n",
        "avg_specificity = np.mean(specificity_list)\n",
        "avg_mcc = np.mean(mcc_list)\n",
        "\n",
        "# Calculate average AUC across all folds\n",
        "avg_auc = np.mean(auc_list)\n",
        "# Print average performance evaluation metrics\n",
        "print(\"Accuracy =\", avg_accuracy)\n",
        "print(\"F1 Score =\", avg_f1)\n",
        "print(\"Precision =\", avg_precision)\n",
        "print(\"Recall =\", avg_recall)\n",
        "print(\"Sensitivity =\", avg_sensitivity)\n",
        "print(\"Specificity =\", avg_specificity)\n",
        "print(\"MCC =\", avg_mcc)\n",
        "print(\"AUC =\", avg_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYyk8J4RA5gu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a77d15a1-f7ef-4f45-f845-41cfa3238397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16/16 [==============================] - 0s 2ms/step\n"
          ]
        }
      ],
      "source": [
        "y_val_pred = model.predict(X_val_fold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEXw52wot5JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b524f7-2e42-4133-fe31-62f21f427a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y_prediction = [0 1 1 1 0 1 0 1 0 1 1 0 1 0 0 0 1 1 0 0 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 1 1\n",
            " 1 0 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 0 0 0 1 1 0 0\n",
            " 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0\n",
            " 0 0 1 1 1 0 1 0 0 1 0 1 0 0 0 1 0 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1\n",
            " 1 1 1 0 0 1 0 1 0 1 1 0 0 0 0 1 1 0 1 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 1\n",
            " 1 1 0 0 0 0 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 1 1 0 0 0 1 0 1\n",
            " 1 1 0 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 0 1 0 1 1 1 0 0 1 1 0 1 0 0 1 1 1 1 1\n",
            " 1 1 1 1 1 0 0 1 1 0 0 1 0 0 1 0 0 0 0 1 1 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 0\n",
            " 1 0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0\n",
            " 1 0 0 1 1 1 0 1 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 1 1 0 0 0 1 0 1 0 0 1 0 1\n",
            " 0 1 1 0 0 0 1 1 1 0 1 0 1 1 0 1 1 1 1 0 0 0 1 0 0 0 0 0 0 1 1 1 0 0 1 1 1\n",
            " 1 0 1 1 1 1 0 0 1 0 0 1 0 1 1 1 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0\n",
            " 0 0 0 1 0 1 1 0 0 0 0 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1\n",
            " 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 1 1 0 0 0 0]\n"
          ]
        }
      ],
      "source": [
        "print(\"Y_prediction =\",y_val_fold)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}